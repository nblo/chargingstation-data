# Overview

The projects' goal is to set up a data model for charging station utilization data. The data is gathered from the [chargecloud API](https://www.chargecloud.de/), a German E-Mobility company providing back-end solutions for Charging Point Operators (**abbrev CPOs). 

Additionally, in order to be able to analyze the relationship between charging stations' utilization and its surroundings [OpenStreetMap](https://www.openstreetmap.org) (*abbrev. OSM*) Points-of-Interest  (*abbrev. POI*) locations like food retailers, shopping malls, and highway services are included in the data model.  

The project serves as a basis for setting up a maintainable and easily extendable data architecture and ETL-process for charging data and it consists of the following steps: 
1. **Data Acquisition** 
    - Call chargecloud API in regular time intervals and store raw results
    - Retrieve OSM Points-of-Interest data for the cities where charging stations are located 
2. **Data Cleaning** 
    - Postprocess API results and transform data master data and utilization data
    - Match POI locations to charging station locations by spatial distance mapping between the two location datasets
3. **Data Modelling and ETL process**
    - Set up data model 
    - Ingest charging data and POI data into Data Warehouse (Redshift)

Some example use cases for the data: 
- business reporting: generate utilization reports for single charging stations, cities or operators
- BI-dashboards: Generate interactive visualizations of charging station usage
- Utilization analysis and predictive modelling: Use utilization data to examine the relationship between utilization and POIs in the charging stations' vicinity. Predict usage development or determine optimal charging point locations with Machine Learning Models

The project was part of the Udacity Data Engineering Nanodegree.  

# How to Run This Project

1. Install packages in `requirements.txt`
2. Provide credentials for Redshift and S3 URLs (optional) in `config.cfg`
3. [Data Acquisition](#Step-1:-Data-Acquisition): 
    - Run command `python get_chargecloud_data.py` inside `./src` folder: Calling chargecloud API in regular time intervals
    - Run command `python get_chargecloud_data.py` inside `./src` folder: Obtaining relevant OSM POI locations
4. [Data Preprocessing](#Step-2:-Data-Cleaning): 
    - Run command `python preprocess_results.py` inside `./src` folder: Preprocessing of API results and spatial matching with OSM data
5. [Data Modelling and ETL](#Step-3:-Data-Modelling-and-Ingestion): 
    - Run command `python etl.py` inside `./src` folder: Creating data model and ingesting data into redshift


Each script can be run independently of each other, as sample datasets are included in `data` folder. 

The S3-bucket specified in the `config.cfg` contains data which was collected from the end of September 2021 to the beginning of January 2021, preprocessed by the data processing scripts and then manually uploaded to the specified S3-buckets. 

If you would like to run the ETL process with a different data set, please upload the preprocessed data generated by `preprocess_results.py` to an S3-bucket and change the corresponding S3-URLs in the `config.cfg` file.


# Contact

Please feel free to contact me if you have any questions or feedback: 
- [linkedin](https://www.linkedin.com/in/nick-losacker/)
- [email](mailto:nick.losacker@eon.com)


## Basics

The project contains data of electric vehicle charging stations and charging events. Here are a few basics to get familiar with charging infrastructure and its terminology. 

A *charging station (abbrev. cs)* (red box) is a piece of infrastructure at a single location where an electric vehicle's battery can be recharged. 

Each charging station consists of one or more *charging points (abbrev. cp)* (blue boxes), where a single electric vehicle can recharge at any given time.

Each charging point has one or more *connectors (abbrev. conn)* (green boxes) in order to satisfy different charging standards (e.g. Chademo, CCS, Type 2) or varying charging power levels (e.g. normal charging, fast-charging, ultra-fast charging). 

Charging Station

<img src="chargingstation.png" alt="Charging Station" width="400"/>

---
Each charging station with its charging points and connectors has static/semi-static master data. Examples of master data are the charging stations' location, address, operator, or the connectors' maximum power level. 


Each charging point and connector also has dynamic occupancy or status data, e.g. if the charging point is occupied, reserved, 
free or out-of-order.

The combination of static and dynamic data is used by car infotainment systems and apps to navigate the user to the nearest  
free and functional charging station. 

# Step 1: Data Acquisition
Script `get_chargecloud_data.py`: Call chargecloud API in regular time intervals and save raw results
- Obtain charging stations in each city by making http request to `https://new-poi.chargecloud.de/<city>`. Each API result contains dynamic occupancy data and static master data
- Save raw results 

Script `get_chargecloud_data.py`: Retrieve OSM POI data in charging stations' vicinity
- Download POI data in charging stations cities with [osmnx](https://github.com/gboeing/osmnx) library. The POIs are specified with the `TAGS` dictionary containing OSM [tags](https://wiki.openstreetmap.org/wiki/Tags) as key-value pairs. 
- Save results as shapefiles


# Step 2: Data Cleaning 
Script `preprocess_results.py`
1. Preprocessing API results: 
- Extract charging stations' and connectors' dynamic occupancy data from each API results. 
- Extract charging station, charging point and connector master data from last API results. 

2. Spatial matching between charging stations and POIs
- Compute which POIs are in the vicinity of each charging station and create mapping table 

# Step 3: Data Modelling and Ingestion
Data Modeling: `sql.py` 
1. Set up [Data Model](#Data-Model)

- Each table is defined by `DataIngester` object containing the following parameters:
    - `table_name`: name of database table 
    - `drop_table`: SQL statement for dropping table
    - `create_table`: SQL statement for creating table 
    - `populate_table`: SQL statement for populating table with records (e.g. `COPY` or `INSERT`) 
    - `drop_constraints`: SQL statement for dropping constraints
    - `data_test_cases`: list of SQL data quality checks that are run after table creation 

2. Data Ingestion: `etl.py` 
- Executing specified sequence of data ingestion tasks 


# Data Model 

The data model consists of two fact tables
- status information of charging points 
- status information of connectors

and six dimension tables
- charging station master data
- charging point master data 
- connector master data 
- time meta data 
- POI locations 
- mapping table matching POIs to charging stations

![Data Model](er_diagram.png)


## Fact tables 

- `status_chargingpoints`: status of charging point

| column    name   | description           | datatype  |
| :--|:-------------|:-----|
| **id_status_cp**      |  unique identifier of chargingpoint status |varchar |
| id_chargingpoint      |  charging point id |varchar |
| query_time | timestamp of API call  |   timestamptz |
| status_cp |    status of charging point |    varchar |
| status_parkingsensor |   status of parking sensor (if available)  |    varchar |

---
- `status_connectors`: status of connectors

| column    name   | description           | datatype  |
| :--|:-------------|:-----|
| **id_status_connectors**      |  unique identifier of connector status |varchar |
| id_connector      |  connector id |varchar |
| query_time | timestamp of API call    |   timestamptz |
| status_connector |   status of connector  |    varchar |


## Dimension tables 
- `charging_station` charging station master data

| column    name   | description           | datatype  |
| :--|:-------------|:-----|
| **id_cs** |    unique identifier for charging station  |    int |
| name |   charging station name  |    varchar |
| address | charging station address (street + house number)   |    varchar |
| city |   charging station city |    varchar |
| postal_code | charging station postal code |    varchar |
| country | charging station country |    varchar |
| owner | charging station owner (if different than charging point operator) |    varchar |
| roaming | whether roaming is available |    boolean |
| longitude | charging station longitude (EPSG 4326) |    float |
| latitude | charging station latitude (EPSG 4326) |    float |
| operator_name | charging station operator (CPO) |    varchar |
| operator_hotline | charging station operator hotline |    varchar |
| open_24_7 | whether charging station is open 24/7 |    boolean |


---
- `charging_point` charging point master data

| column    name   | description           | datatype  |
| :--|:-------------|:-----|
| **id_cp** |    unique identifier for charging point  |    varchar |
| id_cs |  unique identifier to corresponding charging station  |    int |
| charging_station_position |  description specifying charging station position  |    varchar |
| cp_position |  description specifying charging point position (e.g. left or right charging point)  |    varchar |
| vehicle_type | suitable type of vehicle    |    varchar |
| floor_level |  charging point floor level  |    varchar |

---

- `connector` connector master data

| column    name   | description           | datatype  |
| :--|:-------------|:-----|
| **id_connector** |    unique identifier for connector  |    varchar |
| id_cp |   unique identifier for charging point  |    varchar |
| format |  type of plug (socket or permanently installed cable)  |    varchar |
| power_type |  type of power (AC 1-Phase, AC 3-Phase, DC)  |    varchar |
| ampere |  maximum amperage  |    int |
| voltage |  nominal voltage   |    int |
| max_power |  maximum charging power  |    int |
| standard |   charging standard (Chademo, IEC 62196) |    varchar |


---

- `time` time metadata

| column    name   | description           | datatype  |
| :--|:-------------|:-----|
| **query_time** |   time of API call |    timestamptz |
| hour |   timestamp hour of day  |    int4 |
| day |  timestamp day of month  |    int4 |
| week |   timestamp week of year|    int4 |
| month | timestamp month  |    int4 |
| year |  timestamp  year |    int4 |
| weekday |  timestamp day of week  |    int4 |


---

- `poi` OSM POI locations

| column    name   | description           | datatype  |
| :--|:-------------|:-----|
| **id_poi** |   OSM id (combination of osm-type and osm id) |    varchar |
| geom |   POI geometry |    geometry |
| longitude |  POI longitude (EPSG 4326)  |    float |
| latitude |   POI latitude (EPSG 4326)    |    float |
| poi_category |   timestamp week of year|    varchar |

---

- `mapping_poi_cs` Mapping table between POIs and charging stations based on distance matching

| column    name   | description           | datatype  |
| :--|:-------------|:-----|
| id_poi|   OSM id (combination of osm-type and osm id) |    varchar |
| id_cs |   unique identifier for charging station  |    int |


# Design Choices 
Data Lake vs. Data Warehouse
 - For this project data is ingested into a data warehouse with a traditional ETL process. This design was chosen instead of a more flexible Data Lake with ELT process, as the data is (mostly) structured
and both data volume and data variety in big data terms are on the lower end.


Redshift is used as a Data Warehouse technology due to the following reasons: 
- advanced PostGres features like JSON support or advanced spatial database features (PostGIS) are not essential 
- due to data volume easily scalable and managed data warehouse was required
 
Batch Processing vs. Streaming. 
- Possible use cases for this data are analyzing trends in user behavior or occupancy with 
descriptive statistics or as a basis for a machine learning model for predicting optimal charging station locations. Thus, data is ingested in daily batch updates instead of streaming data, as the occupancy data is not as time critical 


Ingesting raw data into Redshift allows for flexible extension or refactoring of data model

Separation each step of in ETL process (Data Acquisition, Data Cleaning, Data Modelling and Data Ingestion) allows for some or all 
of those steps to be transferred to Airflow or AWS

# Next Steps 

Here are the next steps which would improve the project, but were out of scope of the capstone project 
- create charging events table from occupancy data for computing number of charging events, charging station availability or approximate energy transfer
- deal with additional columns not yet implemented in the data model (e.g. charging stations' `opening_hours` or charging points' `capabilities`)
- deal with slowly changing dimension tables (SCD)
- implement data acquisition, batch processing and ETL in Airflow or AWS
- develop a dashboard based on data model 
- expand the number of POI categories considered
- add visualization of charging station utilization data
- automatically upload preprocessed data to specified S3 bucket


# Lessons Learned

Things I would have done differently the next time around: 
- make data i/o agnostic to where files are physically located (local vs. cloud) with package like [pyfilesystems](https://docs.pyfilesystem.org/en/latest/guide.html#why-use-pyfilesystem)  
- immediately implement data acquisition, preprocessing and ETL in Airflow 
- with real life problem always start thinking what business problem you are trying to solve and model data accordingly 
- always include some sort of visualizing data 


# Addressing other scenarios 

- Currently the gathered data results in ingesting roughly 275 million rows a year or about 750k rows per day if API is called in regular 10 minutes intervals.
If the data volume was to be increased by 100x (e.g by calling the API every 6 seconds instead of 10 minutes) it would result in 28 billion rows per year or 75 million rows per day. Due to its flexible horizontal or vertical scaling Redshift would still be a viable option. Data could be ingested every 2-3 hours instead of once or twice a day to avoid writing a huge amount of data at once.

- If the pipeline would be run on a daily basis I would consider Airflow as an orchestration tool. 
There could be different DAGs for ingestion jobs on different scheduling intervals: 
    - chargecloud API call (e.g. every 10 minutes)
    - batch data ingestion into status and master data tables (once or twice a day)
    - batch data ingestion of OSM data (once or twice a month)

    AWS services like AWS step and lambda functions could be a viable alternative. 

- If the database needed to accessed by 100+ people, Redshift allows for sufficient horizontal and vertical scaling with low latency and high concurrency. It could be beneficial to use the API query time as a distribution key, since recent data (last week or last month) is probably more business relevant than historic data. To allow for batch downloading of historic data (last years) building a separate self-service API take load off the Data Warehouse.    
 

# Data Visualization
Example of how to visualize utilization of carging points: 
Still Work-in-Progress.
![Example dashboard](example_dashboard.PNG)


# Acknowledgements 

- [Data-Engineering Template by JPHaus](https://github.com/JPHaus/data-engineering-project-template)
